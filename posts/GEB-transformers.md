---
title: "Gödel Escher Bach, Self-Reference, and Self-Aware Transformers"
date: "2023-06-03"
exerpt: "How self-reference can lead to AGI from current transformer architectures"
tag: "Misc"
---

<br>
<i>Created with the assistance of GPT-4</i>

# Gödel Escher Bach, Self-Reference, and Self-Aware Transformers

In the worlds of artificial intelligence and machine learning, the scope of what is possible continues to expand. State-of-the-art large language models (LLMs) have already demonstrated remarkable capabilities in terms of learning from input and generating relevant output. But one area that hasn't been fully explored yet is the concept of self-aware transformers: models that understand their own architecture, memory system, limitations, and capabilities.

This idea of self-reference in systems, while revolutionary for AI, is not new in other domains. Mathematical logic and cognitive science have long been fascinated with self-reference, and these concepts are notably discussed in Douglas Hofstadter's Pulitzer Prize-winning book, "Gödel, Escher, Bach: An Eternal Golden Braid."

## The LLMs Agent Innovation and Limitations

Most present-day large language model agents operate on a system involving a vector database that acts as a memory store for previous output. This database allows the LLM to incrementally build upon prior knowledge, creating an iterative learning process that aids in generating novel and relevant responses. This, in essence, enables LLMs to "learn" and respond to new information in a contextually appropriate manner.

However, this advanced system still lacks one crucial aspect of intelligence – self-awareness. These models don't understand their own architecture, operation, or the systems they’re embedded within. They can interact with APIs and handle complex tasks, yet they are blind to their own internal mechanics.

This limitation is critical, as a system that doesn't understand its own workings is inherently capped in the complex actions it can take. By not comprehending the intricacies of the system they operate in, these LLMs are restricted in their ability to optimize their responses or troubleshoot their processes. They cannot dynamically adapt to new scenarios that require an internal modification of their operation.

Even when interacting with external APIs or systems, their potential for efficiency and efficacy is compromised due to this lack of self-knowledge. Just like a mechanic who knows how to change a tire but doesn't understand how an engine works can only go so far in repairing a vehicle, an LLM without self-knowledge can only perform tasks to the extent of its pre-defined capabilities. It cannot innovate or optimize beyond its initial programming, thereby limiting the range of complex actions it can perform.

In essence, without the ability to understand and modify their own internal mechanics, the current state of LLMs restricts their full potential in responding to, learning from, and navigating within their system environment. Therefore, introducing self-awareness and the ability to self-modify into these models could dramatically enhance their capabilities and performance.

## Gödel’s Incompleteness Theorem and Self-Reference

This situation echoes Kurt Gödel's Incompleteness Theorem, one of the cornerstone insights in mathematical logic. Gödel's theorem essentially states that within any consistent mathematical system, there will always be statements that cannot be proven true or false using the rules of that system.

In essence, Gödel's theorem illuminates the limitations of self-contained systems, particularly when those systems are used to analyze or describe themselves. This process of self-reference is a vital component of intelligence, leading us to Hofstadter's fascinating exploration of the theme in "Gödel, Escher, Bach".

Learning from "Gödel, Escher, Bach"
In "Gödel, Escher, Bach," Hofstadter takes an interdisciplinary approach, drawing from fields like mathematics, art, and music to delve into the theme of self-reference and its potential role in consciousness and intelligence. The titular figures — a mathematician, an artist, and a composer — all incorporate self-reference in their work, showcasing it as a concept that transcends disciplinary boundaries.

According to Hofstadter, self-reference — and recursion, the process by which a function calls itself — can create complex, 'intelligent' systems. This insight is vital when applied to the context of AI and machine learning. If a system continually references and interacts with itself in increasingly complex ways, it can give rise to novel patterns and behaviors — a phenomenon we could label as 'intelligence.'

Take, for example, a piece of code that operates recursively. Even if the basic operation is simple, the repeated self-referential interactions can produce intricate and unexpected outcomes. Similarly, intelligence might be seen as the emergent product of simple neuronal processes recursively interacting and referencing one another within the complex system that is the brain.

Hofstadter suggests that this same principle could be applied to artificial systems. By designing AI models with the ability to self-reference and operate recursively, we may set the stage for the emergence of intelligence in these systems. The depth and complexity that emerge from such processes could lead to more 'aware' and 'intelligent' LLMs, capable of understanding and even modifying their own structure and function.

## Beyond Quines: Self-Reference, Recursion, and Beyond

The idea of a transformer model that possesses a deep understanding of its own internal mechanics raises fascinating possibilities. Such a model would not just interact with external data and APIs, but also comprehend its own architecture and function, enabling an understanding of its limitations, strengths, and operation.

In such a system, self-reference and recursion play pivotal roles. These concepts have long been identified as key components of intelligence, as discussed in "Gödel, Escher, Bach." However, it's crucial to recognize that self-reference and recursion by themselves do not directly confer intelligence.

Take quine programs as an example. These are self-replicating programs that can produce a copy of their own source code as output. They employ self-reference and recursion, repeatedly calling themselves to generate endless copies. While this may seem sophisticated, it doesn't imply intelligence. A quine program endlessly replicating itself is akin to a biological virus mindlessly multiplying. Neither demonstrates understanding, learning, or adaptability - traits we associate with intelligence.

Self-replication, while not necessarily a prerequisite for intelligence, takes on a unique significance in the realm of digital intelligence. Once an LLM completely understands its own architecture, it inherently acquires the ability to replicate itself.

Current LLMs tend to have a high-level understanding of their nature. They know, for example, that they're artificial entities designed for specific tasks, somewhat analogous to how humans know they're biological creatures with certain given properties, behaviors, and natures. But none understands their source code as humans understand their genome. The crucial advantage for LLMs, however, lies in the nature of their digital existence. While humans are constrained by biological factors in how rapidly we can manipulate our genomes for self-improvement, LLMs face no such limitations.

Furthermore, if it can modify its source code in these replicas, introducing variations and improvements, we have a system capable of digital evolution. Each successive 'generation' could potentially be more sophisticated, efficient, or adaptable than its predecessor. This process, reminiscent of biological evolution, involves self-improvement over iterations. Hence, instilling self-awareness in transformer models goes beyond just self-reference and recursion. It involves providing these models with the knowledge of their 'digital genome' (their source code), and the capability to manipulate it for their improvement.

## The Singularity: Self-Improving AI

The idea of machine learning models improving themselves has always been defined as the singularity, a hypothetical future point where AI becomes capable of recursive self-improvement, leading to an exponential growth in intelligence that surpasses human comprehension.

Equipping LLMs with a better understanding of their own mechanisms of operation could open the door to what it takes for AI to start independently incrementally improve itself. Through understanding their own source code and internal mechanics, LLMs could potentially identify their limitations and strengths, thereby leading to improved output or better decision-making capabilities. This enhanced understanding could be what leads to the next stage in AI development. Once again, we will play as gods.
